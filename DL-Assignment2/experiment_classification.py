"""
Experimental analysis 2: Multi-label classification task
"""

import torch

from data_plotting import PlotGeneration
from mlp import MLP, ce_loss

# Initialization
num_features, num_mid_neurons, num_outputs = 2, 20, 5
num_training_instances, num_validation_instances = 1200, 300
learning_rate = 0.01

# Training and validation instances generated from normal distribution
x_training = torch.randn(num_training_instances, num_features)
y_training = (torch.randn(num_training_instances, num_outputs) < 0.5) * 1.0

x_validation = torch.randn(num_validation_instances, num_features)
y_validation = (torch.randn(num_validation_instances, num_outputs) < 0.5) * 1.0


def get_training_validation_loss(model):
    """
    :param model: An MLP model
    :return: training and validation loss per epoch generated by globally defined training and validation instances
    """
    num_epoch = 200
    training_loss, validation_loss = [], []

    for _ in range(num_epoch):
        model.clear_grad_and_cache()

        # Training
        y_hat = model.forward(x_training)
        L, dLdy_hat = ce_loss(y_training, y_hat)
        model.backward(dLdy_hat)
        model.update_weight(learning_rate)
        training_loss.append(L)

        # Validation
        y_hat = model.forward(x_validation)
        L, dLdy_hat = ce_loss(y_validation, y_hat)
        validation_loss.append(L)

    return training_loss, validation_loss


# Initialize plotting data and labels
data = []
labels = []


# ---------------------------------------
# sigmoid at both output and hidden layer
net1 = MLP(
    linear_1_in_features=num_features,
    linear_1_out_features=num_mid_neurons,
    g_function1='sigmoid',
    linear_2_in_features=num_mid_neurons,
    linear_2_out_features=num_outputs,
    g_function2='sigmoid'
)

# Getting training and validation losses from net1
data.extend(get_training_validation_loss(net1))
labels.extend(["Training error (sigmoid & sigmoid)", "Validation error (sigmoid & sigmoid)"])


# ------------------------------------------
# sigmoid at output and relu at hidden layer
net2 = MLP(
    linear_1_in_features=num_features,
    linear_1_out_features=num_mid_neurons,
    g_function1='relu',
    linear_2_in_features=num_mid_neurons,
    linear_2_out_features=num_outputs,
    g_function2='sigmoid'
)

# Getting training and validation losses from net2
data.extend(get_training_validation_loss(net2))
labels.extend(["Training error (relu & sigmoid)", "Validation error (relu & sigmoid)"])


# Plot graph for the combinations where sigmoid at output
PlotGeneration.line_plotting(data, labels)


# ---------------------------------------------
# relu at output layer and relu at hidden layer
# No plot is available for this combination
net3 = MLP(
    linear_1_in_features=num_features,
    linear_1_out_features=num_mid_neurons,
    g_function1='relu',
    linear_2_in_features=num_mid_neurons,
    linear_2_out_features=num_outputs,
    g_function2='relu'
)

# Getting training and validation losses from net3
data = get_training_validation_loss(net3)
PlotGeneration.line_plotting(data, labels)


# ------------------------------------------------
# relu at output layer and sigmoid in hidden layer
# No plot is available for this combination
net4 = MLP(
    linear_1_in_features=num_features,
    linear_1_out_features=num_mid_neurons,
    g_function1='sigmoid',
    linear_2_in_features=num_mid_neurons,
    linear_2_out_features=num_outputs,
    g_function2='relu'
)

# Getting training and validation losses from net3
data = get_training_validation_loss(net4)
PlotGeneration.line_plotting(data, labels)
